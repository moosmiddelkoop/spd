# Single Layer Memorization Model
# --- WandB ---
wandb_project: nathu-memorization-spd
wandb_run_name: null
wandb_run_name_prefix: ""

# --- General ---
seed: 0
C: 500
n_mask_samples: 1
gate_type: "mlp"
gate_hidden_dims: [128]
sigmoid_type: "leaky_hard"
target_module_patterns:
  - "linear"
  - "output"

# --- Loss Coefficients ---
faithfulness_coeff: 1.0
out_recon_coeff: 0.0
recon_coeff: null
stochastic_recon_coeff: 1.0
recon_layerwise_coeff: null
stochastic_recon_layerwise_coeff: 1.0
importance_minimality_coeff: 3e-3
pnorm: 2
p_anneal_start_frac: 0.25
p_anneal_final_p: 1
output_loss_type: mse

# --- Training ---
batch_size: 2048
steps: 100_000
lr: 1e-3
lr_schedule: cosine
lr_warmup_pct: 0.1
n_eval_steps: 100

# --- Logging & Saving ---
image_freq: 5_000
image_on_first_step: true
print_freq: 500
save_freq: null

# --- Pretrained model info ---
pretrained_model_class: "spd.experiments.memorization.models.SingleLayerMemorizationMLP"
pretrained_model_path: "wandb:goodfire/spd-gf-spd_experiments_memorization/runs/gkqn4tc1"

# --- Task Specific ---
task_config:
  task_name: memorization