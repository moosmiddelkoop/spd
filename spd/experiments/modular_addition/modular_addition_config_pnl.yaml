# --- WandB ---
wandb_project: spd
wandb_run_name: null
wandb_run_name_prefix: ""

# --- General ---
seed: 0
C: 300
n_mask_samples: 1
gate_type: "mlp"
gate_hidden_dims: [128]
sigmoid_type: "leaky_hard"
target_module_patterns: [
  "embed.embedding",           # Token embedding
  "pos_embed.pos_embedding",   # Positional embedding  
  "*.q_proj",                  # Query projections for all attention heads
  "*.k_proj",                  # Key projections for all attention heads
  "*.v_proj",                  # Value projections for all attention heads
  "*.o_proj",                  # Output projections for all attention heads
  "*.W_in",                    # MLP input layers
  "*.W_out",                   # MLP output layers
  "unembed.linear"             # Output unembedding layer
]

# --- Loss Coefficients ---
faithfulness_coeff: 2.0
recon_coeff: null
stochastic_recon_coeff: 1.0
recon_layerwise_coeff: null
stochastic_recon_layerwise_coeff: 1.0
importance_minimality_coeff: 1e-6
schatten_coeff: null
out_recon_coeff: 0.0
embedding_recon_coeff: null
is_embed_unembed_recon: false
pnorm: 2.0
p_anneal_start_frac: 0.25
p_anneal_final_p: 1
output_loss_type: kl_final_only

# --- Training ---
batch_size: 1024
steps: 200_000
lr: 5e-4
lr_schedule: cosine
lr_warmup_pct: 0.01
lr_exponential_halflife: null
n_eval_steps: 100

# --- Logging & Saving ---
image_freq: 5000
image_on_first_step: true
print_freq: 500
save_freq: null
figures_fns: ["ci_histograms", "mean_component_activation_counts", "modular_addition_ci_plots"]
metrics_fns: ["ci_l0", "modular_addition_task_performance"]

# --- Pretrained model info ---
pretrained_model_class: "spd.experiments.modular_addition.vendored.transformers.Transformer"
pretrained_model_path: null  # Will be set to checkpoint path in script
pretrained_model_name_hf: null
pretrained_model_output_attr: null
tokenizer_name: null

# --- Task Specific ---
task_config:
  task_name: modular_addition